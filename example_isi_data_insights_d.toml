[global]
# Config file version
# mandatory field
# This field ties the config file syntax/format back to the collector version.
# This allows the collector to flag breaking changes where the config file needs to be updated.
# string optionally starting with "v"/"V" e.g., "v0.10", or "0.10"
version = "v0.29"

logfile = "gostats.log"
log_to_stdout = false

# Pluggable back end support
# Supported back ends are "influxdb", "influxdbv2", "prometheus" and "discard"
# Default configuration uses InfluxDB (v1)
stats_processor = "influxdb"

# Maximum number of retries in case of errors during write to stat_processor
# Default is 8 retries. Uncomment the following line to retry forever
# stats_processor_max_retries = 0

# The stats_processor_retry_interval parameter provides the ability to override the
# minimum interval that the daemon will retry in case writing to the stats_processor fails.
# Default is 5 second. Uncomment the following line to start with a 1 second interval.
# stats_processor_retry_interval = 1

# Maximum number of retries for http requests (both data and auth)
# Default is 8 retries. Uncomment the following line to retry forever
# max_retries = 0

# The min_update_interval_override parameter provides the ability to override the
# minimum interval that the daemon will query for a set of stats.
# By default, the collector queries the API and collects the stats in buckets based
# on the stat metadata update interval.
# If there is concern over the load presented by collection, or a less frequent
# collection interval is desired/required, this value can be increased.
# The minimum permitted (and default) stat update interval is 5 seconds
min_update_interval_override = 5

# preserve case of cluster names to lowercase, defaults to false.
# preserve_case = true

# Specifies the active list of stat groups to query, each stat group name
# specified here should have a corresponding section in the config file.
active_stat_groups = [
    "cluster_cpu_stats",
    "cluster_network_traffic_stats",
    "cluster_client_activity_stats",
    "cluster_health_stats",
    "ifs_space_stats",
    "ifs_rate_stats",
    "node_load_stats",
    "node_disk_stats",
    "node_net_stats",
    "cluster_disk_rate_stats",
    "cluster_proto_stats",
    "cache_stats",
    "heat_total_stats",
]

############################ End of global section ############################

############################ Back end configuration ###########################
# Influxdb configuration
[influxdb]
host = "localhost"
port = "8086"
database = "isi_data_insights"
authenticated = false
# username = "influxuser"
# password = "influxpass"
# or e.g.
# password = "$env:INFLUXPASS"

# Influxdbv2 configuration
[influxdbv2]
host = "localhost"
port = "8086"
org = "my-org"
bucket = "isi_data_insights"
access_token = "<access_token>"
# or e.g.
# access_token = "$env:INFLUX_TOKEN"

# Prometheus configuration
[prometheus]
# optional basic auth
authenticated = false
# username = "promuser"
# password = "prompass"
# tls_cert = "/path/to/certificate"
# tls_key = "/path/to/key"

# discard back end currently has no configurable options and hence no config stanza

######################## End of back end configuration ########################

# If using prometheus, the collector supports the Prometheus "http SD" service
# discovery mechanism.
#
# The hostname/IP for the discovery service can be hard coded via listen_addr below
# otherwise the code will attempt to find and external public IP address
[prom_http_sd]
enabled = false
# listen_addr = "external_hostname"
sd_port = 9999

############################# Cluster configuration ###########################

# clusters in this section are queried for all stat groups
# [[cluster]]
# hostname = "mycluster.xyz.com"
# username = "statsuser"
# password = "sekr1t"
# verify-ssl = false
# authtype = "basic-auth"
# disabled = false
# prometheus_port = 9090
# preserve_case = true
#	...
[[cluster]]
hostname = "demo.cluster.com"
username = "root"
password = "a"
# or e.g.
# password = "$env:CLUSTER1PASS"
verify-ssl = true

######################### End of cluster configuration ########################

##################### Summary stat group configuration ########################

[summary_stats]
protocol = false
client = false

################## End of summary stat group configuration ####################

############################ Stat group definitions ###########################

# Definitions of the various groups of statistics to collect
[[statgroup]]
name = "cluster_cpu_stats"
# update interval is in seconds or use *<number> to base the update interval
# off each stat's collection interval (i.e. *2 == 2 times the collection
# interval, *1 == * == 1 times the collection invterval of each stat)
update_interval = "*"
stats = [
    "cluster.cpu.sys.avg",
    "cluster.cpu.user.avg",
    "cluster.cpu.idle.avg",
    "cluster.cpu.intr.avg",
]

[[statgroup]]
name = "cluster_network_traffic_stats"
update_interval = "*"
stats = [
    "cluster.net.ext.bytes.in.rate",
    "cluster.net.ext.bytes.out.rate",
    "cluster.net.ext.packets.in.rate",
    "cluster.net.ext.packets.out.rate",
    "cluster.net.ext.errors.in.rate",
    "cluster.net.ext.errors.out.rate",
]

[[statgroup]]
name = "cluster_client_activity_stats"
update_interval = "*"
stats = [
    "node.clientstats.active.ftp",
    "node.clientstats.active.hdfs",
    "node.clientstats.active.http",
    "node.clientstats.active.lsass_out",
    "node.clientstats.active.jobd",
    "node.clientstats.active.nfs",
    "node.clientstats.active.nfs4",
    "node.clientstats.active.nlm",
    "node.clientstats.active.papi",
    "node.clientstats.active.siq",
    "node.clientstats.active.cifs",
    "node.clientstats.active.smb2",
    "node.clientstats.connected.ftp",
    "node.clientstats.connected.hdfs",
    "node.clientstats.connected.http",
    "node.clientstats.connected.nfs",
    "node.clientstats.connected.nlm",
    "node.clientstats.connected.papi",
    "node.clientstats.connected.siq",
    "node.clientstats.connected.cifs",
]

[[statgroup]]
name = "cluster_health_stats"
update_interval = "*"
stats = [
    "cluster.health",
    "cluster.node.count.all",
    "cluster.node.count.down",
]

[[statgroup]]
name = "ifs_space_stats"
update_interval = "*"
stats = [
    "ifs.bytes.avail",
    "ifs.bytes.free",
    "ifs.bytes.used",
    "ifs.bytes.total",
    "ifs.percent.free",
    "ifs.percent.avail",
    "ifs.percent.used",
]

[[statgroup]]
name = "ifs_rate_stats"
update_interval = "*"
stats = [
    "ifs.bytes.in.rate",
    "ifs.bytes.out.rate",
    "ifs.ops.in.rate",
    "ifs.ops.out.rate",
]

[[statgroup]]
name = "node_load_stats"
update_interval = "*"
stats = [
    "node.cpu.throttling",
    "node.load.1min",
    "node.load.5min",
    "node.load.15min",
    "node.memory.used",
    "node.memory.free",
    "node.open.files",
]

[[statgroup]]
name = "node_disk_stats"
update_interval = "*"
stats = [
    "node.disk.bytes.out.rate.avg",
    "node.disk.bytes.in.rate.avg",
    "node.disk.busy.avg",
    "node.disk.xfers.out.rate.avg",
    "node.disk.xfers.in.rate.avg",
    "node.disk.xfer.size.out.avg",
    "node.disk.xfer.size.in.avg",
    "node.disk.access.latency.avg",
    "node.disk.access.slow.avg",
    "node.disk.iosched.queue.avg",
    "node.disk.iosched.latency.avg",
]

[[statgroup]]
name = "node_net_stats"
update_interval = "*"
stats = [
    "node.net.int.bytes.in.rate",
    "node.net.int.bytes.out.rate",
    "node.net.ext.bytes.in.rate",
    "node.net.ext.bytes.out.rate",
    "node.net.int.errors.in.rate",
    "node.net.int.errors.out.rate",
    "node.net.ext.errors.in.rate",
    "node.net.ext.errors.out.rate",
]

[[statgroup]]
name = "cluster_disk_rate_stats"
update_interval = "*"
stats = [
    "cluster.disk.xfers.rate",
    "cluster.disk.xfers.in.rate",
    "cluster.disk.xfers.out.rate",
    "cluster.disk.bytes.in.rate",
    "cluster.disk.bytes.out.rate",
]

[[statgroup]]
name = "cluster_proto_stats"
update_interval = "*"
stats = [
    "cluster.protostats.nfs",
    "cluster.protostats.nlm",
    "cluster.protostats.cifs",
    "cluster.protostats.ftp",
    "cluster.protostats.http",
    "cluster.protostats.siq",
    "cluster.protostats.jobd",
    "cluster.protostats.smb2",
    "cluster.protostats.nfs4",
    "cluster.protostats.irp",
    "cluster.protostats.lsass_in",
    "cluster.protostats.lsass_out",
    "cluster.protostats.papi",
    "cluster.protostats.hdfs",
    "cluster.protostats.nfs.total",
    "cluster.protostats.nlm.total",
    "cluster.protostats.cifs.total",
    "cluster.protostats.ftp.total",
    "cluster.protostats.http.total",
    "cluster.protostats.siq.total",
    "cluster.protostats.jobd.total",
    "cluster.protostats.smb2.total",
    "cluster.protostats.nfs4.total",
    "cluster.protostats.irp.total",
    "cluster.protostats.lsass_in.total",
    "cluster.protostats.lsass_out.total",
    "cluster.protostats.papi.total",
    "cluster.protostats.hdfs.total",
]

[[statgroup]]
name = "cache_stats"
update_interval = "*"
stats = [
    "node.ifs.cache",
]

[[statgroup]]
name = "heat_total_stats"
update_interval = "*"
stats = [
    "node.ifs.heat.lock.total",
    "node.ifs.heat.blocked.total",
    "node.ifs.heat.contended.total",
    "node.ifs.heat.deadlocked.total",
    "node.ifs.heat.write.total",
    "node.ifs.heat.read.total",
    "node.ifs.heat.lookup.total",
    "node.ifs.heat.rename.total",
    "node.ifs.heat.link.total",
    "node.ifs.heat.unlink.total",
    "node.ifs.heat.getattr.total",
    "node.ifs.heat.setattr.total",
]

######################### End of stat group definitions #######################