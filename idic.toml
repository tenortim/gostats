[global]
# Parameters specified on the command line will supersede the parameters
# in this section.
# pid_file: /var/run/isi_data_insights_d.pid
# log_file: /var/run/isi_data_insights_d.log
# default log_level is INFO
# log_level = "DEBUG"
stats_processor = "influxdb_plugin"
# The first three arguments are the host/IP, port and database for the InfluxDB database
# Optionally, pass the InfluxDB username and password as the fourth and fifth arguments, e.g.:
# localhost 8086 isi_data_insights influxdbuser influxdbpass
stats_processor_args = [ "localhost", "8086", "isi_data_insights" ]

# Alternative back end for testing
# stat_processor = "discard_plugin"
stat_processor_args = []

# If set to 0 or -1, authentication will be retried indefintely
# There is still an exponential backoff that is clamped to a maximum of
# 30 minutes.
# Default is still 8 retries. Uncomment the following line to retry forever
# max_retries = 0

# Specifies the active list of stat groups to query, each stat group name
# specified here should have a corresponding section in the config file.
active_stat_groups = [
    "cluster_cpu_stats",
    "cluster_network_traffic_stats",
    "cluster_client_activity_stats",
    "cluster_health_stats",
    "ifs_space_stats",
    "ifs_rate_stats",
    "node_load_stats",
    "node_disk_stats",
    "node_net_stats",
    "cluster_disk_rate_stats",
    "cluster_proto_stats",
    "cache_stats",
    "heat_total_stats",
]

# The min_update_interval_override param provides ability to override the
# minimum interval that the daemon will query for a set of stats. The purpose
# of the minimum interval, which defaults to 30 seconds, is to prevent
# the daemon's queries from putting too much stress on the cluster.
# The default value is 30 seconds.
#
# The most-frequent stat update interval is 5 seconds and that does not seem to
# present excessive load in internal testing, so set that here.
min_update_interval_override = 5

# clusters in this section are queried for all stat groups
# clusters: [username1:password1@]<ip-or-host-address1>[:True|False]
#	[[username2:password2]@<ip-or-host-address2>[:True|False]]
#	[[username3:password3]@<ip-or-host-address3>[:True|False]]
#	...
# If you don't specify the username and password then you will be prompted
# for them when the daemon starts up.
# Use the optional True or False on the end to specify whether the cluster's
# SSL certificate should be verified. If it is omitted then the default is
# False (i.e. don't verify SSL cert).
[[cluster]]
hostname = "cse-s210-1.cse.isilon.com"
username = "root"
password = "a"
verify-ssl = false

# Definitions of the various groups of statistics to collect
[[statgroup]]
name = "cluster_cpu_stats"
# The clusters (optional) param defines a list of clusters specific to this
# group.
# clusters: 10.25.69.74 10.25.69.75
# update interval is in seconds or use *<number> to base the update interval
# off each stat's collection interval (i.e. *2 == 2 times the collection
# interval, *1 == * == 1 times the collection invterval of each stat)
update_interval = "*"
stats = [
    "cluster.cpu.sys.avg",
    "cluster.cpu.user.avg",
    "cluster.cpu.idle.avg",
    "cluster.cpu.intr.avg",
]

[[statgroup]]
name = "cluster_network_traffic_stats"
update_interval = "*"
stats = [
    "cluster.net.ext.bytes.in.rate",
    "cluster.net.ext.bytes.out.rate",
    "cluster.net.ext.packets.in.rate",
    "cluster.net.ext.packets.out.rate",
    "cluster.net.ext.errors.in.rate",
    "cluster.net.ext.errors.out.rate",
]

[[statgroup]]
name = "cluster_client_activity_stats"
update_interval = "*"
stats = [
    "node.clientstats.active.ftp",
    "node.clientstats.active.hdfs",
    "node.clientstats.active.http",
    "node.clientstats.active.lsass_out",
    "node.clientstats.active.jobd",
    "node.clientstats.active.nfs",
    "node.clientstats.active.nfs4",
    "node.clientstats.active.nlm",
    "node.clientstats.active.papi",
    "node.clientstats.active.siq",
    "node.clientstats.active.cifs",
    "node.clientstats.active.smb2",
    "node.clientstats.connected.ftp",
    "node.clientstats.connected.hdfs",
    "node.clientstats.connected.http",
    "node.clientstats.connected.nfs",
    "node.clientstats.connected.nlm",
    "node.clientstats.connected.papi",
    "node.clientstats.connected.siq",
    "node.clientstats.connected.cifs",
]

[[statgroup]]
name = "cluster_health_stats"
update_interval = "*"
stats = [
    "cluster.health",
    "cluster.node.count.all",
    "cluster.node.count.down",
]

[[statgroup]]
name = "ifs_space_stats"
update_interval = "*"
stats = [
    "ifs.bytes.avail",
    "ifs.bytes.free",
    "ifs.bytes.used",
    "ifs.bytes.total",
    "ifs.percent.free",
    "ifs.percent.avail",
    "ifs.percent.used",
]

[[statgroup]]
name = "ifs_rate_stats"
update_interval = "*"
stats = [
    "ifs.bytes.in.rate",
    "ifs.bytes.out.rate",
    "ifs.ops.in.rate",
    "ifs.ops.out.rate",
]

[[statgroup]]
name = "node_load_stats"
update_interval = "*"
stats = [
    "node.cpu.throttling",
    "node.load.1min",
    "node.load.5min",
    "node.load.15min",
    "node.memory.used",
    "node.memory.free",
    "node.open.files",
]

[[statgroup]]
name = "node_disk_stats"
update_interval = "*"
stats = [
    "node.disk.bytes.out.rate.avg",
    "node.disk.bytes.in.rate.avg",
    "node.disk.busy.avg",
    "node.disk.xfers.out.rate.avg",
    "node.disk.xfers.in.rate.avg",
    "node.disk.xfer.size.out.avg",
    "node.disk.xfer.size.in.avg",
    "node.disk.access.latency.avg",
    "node.disk.access.slow.avg",
    "node.disk.iosched.queue.avg",
    "node.disk.iosched.latency.avg",
]

[[statgroup]]
name = "node_net_stats"
update_interval = "*"
stats = [
    "node.net.int.bytes.in.rate",
    "node.net.int.bytes.out.rate",
    "node.net.ext.bytes.in.rate",
    "node.net.ext.bytes.out.rate",
    "node.net.int.errors.in.rate",
    "node.net.int.errors.out.rate",
    "node.net.ext.errors.in.rate",
    "node.net.ext.errors.out.rate",
]

[[statgroup]]
name = "cluster_disk_rate_stats"
update_interval = "*"
stats = [
    "cluster.disk.xfers.rate",
    "cluster.disk.xfers.in.rate",
    "cluster.disk.xfers.out.rate",
    "cluster.disk.bytes.in.rate",
    "cluster.disk.bytes.out.rate",
]

[[statgroup]]
name = "cluster_proto_stats"
update_interval = "*"
stats = [
    "cluster.protostats.nfs",
    "cluster.protostats.nlm",
    "cluster.protostats.cifs",
    "cluster.protostats.ftp",
    "cluster.protostats.http",
    "cluster.protostats.siq",
    "cluster.protostats.jobd",
    "cluster.protostats.smb2",
    "cluster.protostats.nfs4",
    "cluster.protostats.irp",
    "cluster.protostats.lsass_in",
    "cluster.protostats.lsass_out",
    "cluster.protostats.papi",
    "cluster.protostats.hdfs",
    "cluster.protostats.nfs.total",
    "cluster.protostats.nlm.total",
    "cluster.protostats.cifs.total",
    "cluster.protostats.ftp.total",
    "cluster.protostats.http.total",
    "cluster.protostats.siq.total",
    "cluster.protostats.jobd.total",
    "cluster.protostats.smb2.total",
    "cluster.protostats.nfs4.total",
    "cluster.protostats.irp.total",
    "cluster.protostats.lsass_in.total",
    "cluster.protostats.lsass_out.total",
    "cluster.protostats.papi.total",
    "cluster.protostats.hdfs.total",
]

[[statgroup]]
name = "cache_stats"
update_interval = "*"
stats = [
    "node.ifs.cache",
]

[[statgroup]]
name = "heat_total_stats"
update_interval = "*"
stats = [
    "node.ifs.heat.lock.total",
    "node.ifs.heat.blocked.total",
    "node.ifs.heat.contended.total",
    "node.ifs.heat.deadlocked.total",
    "node.ifs.heat.write.total",
    "node.ifs.heat.read.total",
    "node.ifs.heat.lookup.total",
    "node.ifs.heat.rename.total",
    "node.ifs.heat.link.total",
    "node.ifs.heat.unlink.total",
    "node.ifs.heat.getattr.total",
    "node.ifs.heat.setattr.total",
]
